# Casual Thoughts about Linear Algebra

## 1. 数据分析问题的本质（例子：最小二乘法）

Data Science以及与数据分析有关的问题的解决过程往往是要**把一个现实问题转变为一个数学问题，经过求解之后再将这个解回归到现实问题之中**。以一个最小二乘法解决数据回归问题作为例子：假设我们现在有一组数据，每个数据对应一组现实事物的特征以及其归属的群类：例如我们根据个体对二次元手游的偏好程度来推断其所属群类，其特征即为对各个知名IP的评价，例如（原神，明日方舟，重返未来）这样一个特征组附带着一个死宅程度评分，这个数据集中的一个数据点就可以表示为（5，3，2，86），反映这个人对这三个知名IP的评价分数，以及这个人本身的死宅分数 -- 想象一个四维的空间，这个点此刻就在这个空间中以坐标（5，3，2，86）呈现。

假设我们拥有很多个这样的数据点，它们共同组成了我们所需的数据集，而我们的任务则是**基于当前的数据，归纳总结出一个基本普适的规律，使其能够作为预测未知数据的依据**，也就是说，你得到了这个规律，对于一个属于同类群的陌生的人，你也可以猜测他对各项IP的评分。实际上，在这个问题中，数学的味道十分明显--想象这些数据点分布在空间中，怎样才能求解出一个适用于这些数据点的分布规律呢？或许一个函数可以，我们现阶段设想它是一个直线，能够让绝大多数的数据点贴合在它上面，然后，我们在猜测陌生数据的时候，直接将对各个手游的评价分数（x，y，z）代入这个函数，便能得到此人的死宅分数预测score了。

但怎样得出这个函数呢？通过数据点出发，我们能得到很多个方程组成的一个大方程组：

```
ux1 + vy1 + wz1 = score1 
ux2 + vx2 + wz2 = score2
...
```

但这样似乎不太对--我们凭什么断定，这些数据点在数据空间里的位置纯粹由x，y，z来决定呢？我们为什么能断定，没有什么不受这三个IP评分影响的某种东西同样在影响着一个人的死宅程度呢？我们不能断定，因此我们还要加入一个常数项，用来描述在以x，y，z作为特征主导我们的判断的情况下，所遗漏掉的其他影响因素（这些因素不随x，y，z而变，因此可以写为常数），如下所示：

```
b + ux1 + vy1 + wz1 = score1
b + ux2 + vx2 + wz2 = score2
...
```

我们得到了方程组，然后呢？我们自然是要解这个方程组，但我们要得到的并不是x，y，z，score中的任何一个，我们要得到的是构成我们需要的函数的参数，也就是b，u，v，w，这些参数是我们真正要求解的东西。我们的目标也因此转移为了解一个硕大的三元一次方程组。由于这个方程组太过巨大，我们此处先列举上四个，并且列出它们的矩阵形式。

```
b + ux1 + vy1 + wz1 = score1
b + ux2 + vx2 + wz2 = score2
b + ux3 + vy3 + wz3 = score3
b + ux4 + vx4 + wz4 = score4
```

也就是：

```
1 x1 y1 z1   b   score1
1 x2 y2 z2 * u = score2
1 x3 y3 z3   v   score3
1 x4 y4 z4   w   score4
```

它自然可以写作 `Ax = b` 的形式。

显然，如果对矩阵消元，我们是可以得到这个方程组的唯一解的，前提是，这个矩阵的各行各列都线性无关。想象一下，一个可能有数万行列的矩阵，各行各列都线性无关的概率非常小，甚至于大概率是无解的。因此，我们需要稍微改变下我们的目标，我们需要从求解，改为求当前（无解）情况下的最优解。但是何为最优解？最优解便是，当我们把矩阵A当作b，u，v，w到向量score的线性映射，而这个线性映射无解，也就是得不到当前的向量score之后，我们将原本的向量score分解为两个向量err和新的score，这个新的score落在了原本方程组的解域之中，也就是矩阵A的列空间之中，我们用新的score来代替原本不可能达到的那个score，但这个新score此时只能说是替代解而非最优解，如何得到最优解？我们便可以看向量的分解，想让新的score最接近原来的score，就要让另一个分量err最小--而什么时候err最小呢？答案是新的score和err刚好正交的时候，也就是说，新的score是原本score向量在矩阵A列空间上的投影的时候。也就是说，求方程组Ax=b的最优解，即是求b在矩阵A列空间上投影的过程。

那么，在线性代数中，自然有一个求解向量b在矩阵A列空间投影的公式，那便是：p = （A（ATA）-1 AT）b，由此，我们便求出了p，而由其初始形态 x = （ATA）-1 AT b，我们也能得到x，即向量b，u，v，w，问题得解。

解决了b，u，v，w，我们便能得到我们想要的那个函数，从而根据手游评分来预测未知个体的死宅浓度了。

## 2. 矩阵与线性映射

在进行这一部分的讨论之前，我们首先要知道，一个问题，或者一个数学变换，怎样才是“线性”的？

首先，此处我们对数学变换的定义近似为一个函数：有一个数学变换f，它能够接收一个x，并返回一个变换后的y。而在这个有关数学变换的定义的基础上，对于一个线性变换来说，它在加法和数乘运算上封闭，即满足：

```
f(x1 + x2) = f(x1) + f(x2)
f(cx) = cf(x) 
```

以一条经过原点的直线 f(x) = 2x 作为例子，这条直线就是一个线性变换，因为它满足`f(1 + 2) = 2 * 3 = f(1) + f(2) = 2 * 1 + 2 * 3`，并且满足`f(3 * 1) = 3 = 3 * f(1)`，但如果我们将f(x)变为f(x) = 2x + 1，或者f(x) = x^2，这个变换就不在加法和数乘上封闭，也不是一个线性变换了（2x+1的图像虽然是一个直线，但其并不是一个线性变换，而是在经过原点的直线上添加了一个平移，为一个**仿射变换**）。 至于怎样判断一个潜在的数学变换是不是线性变换，相比我们只有先假设它是，而后再根据上面两个性质来反推其存在了吧。

我们从要解决的问题背景中，便能看出某个地方似乎经过了某种“线性变换”。举一个利用线性变换的性质或原理解决现实问题的例子，那便是知名的“鸡兔同笼问题”情景：已知在一个笼子里有若干只鸡，若干只兔子，但我们知道的是笼子中一共有10个头，28只脚（当然，我们还知道一只鸡有1个头2只脚，一只兔子有1个头4只脚），通过这个问题情景，我们的任务便是解出笼子中一共有多少只鸡，多少只兔子。

这个问题中隐含着一个数学变换，而且是线性变换，那便是“一只鸡有一头两脚，一只兔有一头四脚”，即：

```
fchicken(x) = [x, 2x] = [1, 2] * x
frabbit(x) = [x, 4x] = [1, 4] * x
```

甚至于，我们可以跳出鸡和兔的区分，将它们两个的数量以及头，脚数量整合在一起，形成一个更大的线性变换--这也是线性变换的一个特征，通常来说，一个大的线性变换可以被拆分为数个小的线性变换，而多个小的线性变换也可以整合为一个大的线性变换。想要证明它是线性变换也很简单，因为它们在加法和数乘运算上封闭--显然，两只鸡摆在一起不可能会发生三个头五只脚一类的事情，对于兔子也一样，而对着一只鸡数两次头和脚的数量也显然不会突然发现多了一个头或少了一只脚的情况，而当你把鸡和兔一起数，数出来的结果肯定也不会和分开数有所不同。

```
f([chicken, rabbit]) = fchicken(chicken) + frabbit(rabbit)
                     = [1, 2] * chicken + [1, 4] * rabbit
                     = [heads, feet]
```

在线性代数的背景下，这样一个线性变换也会被称作一个**线性映射（Linear Map）**，表示一个向量`[chicken, rabbit]`经过了变换f，从原来所属的向量空间被映射到了另一个向量空间，在此空间中这个向量变为了`[heads, feet]`的模样。而从前面对于`f([chicken, rabbit])`的表示我们可以看出，这个线性变换俨然可以被写作一个矩阵乘以向量的过程，即：

```
A = 
$$\begin{matrix}
1&1\\
2&4\\
\end{matrix}$$
```
