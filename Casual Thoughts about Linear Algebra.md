# Casual Thoughts about Linear Algebra

## 1. 数据分析问题的本质（例子：最小二乘法）

Data Science以及与数据分析有关的问题的解决过程往往是要**把一个现实问题转变为一个数学问题，经过求解之后再将这个解回归到现实问题之中**。以一个最小二乘法解决数据回归问题作为例子：假设我们现在有一组数据，每个数据对应一组现实事物的特征以及其归属的群类：例如我们根据个体对二次元手游的偏好程度来推断其所属群类，其特征即为对各个知名IP的评价，例如（原神，明日方舟，重返未来）这样一个特征组附带着一个死宅程度评分，这个数据集中的一个数据点就可以表示为（5，3，2，86），反映这个人对这三个知名IP的评价分数，以及这个人本身的死宅分数 -- 想象一个四维的空间，这个点此刻就在这个空间中以坐标（5，3，2，86）呈现。

假设我们拥有很多个这样的数据点，它们共同组成了我们所需的数据集，而我们的任务则是**基于当前的数据，归纳总结出一个基本普适的规律，使其能够作为预测未知数据的依据**，也就是说，你得到了这个规律，对于一个属于同类群的陌生的人，你也可以猜测他对各项IP的评分。实际上，在这个问题中，数学的味道十分明显--想象这些数据点分布在空间中，怎样才能求解出一个适用于这些数据点的分布规律呢？或许一个函数可以，我们现阶段设想它是一个直线，能够让绝大多数的数据点贴合在它上面，然后，我们在猜测陌生数据的时候，直接将对各个手游的评价分数（x，y，z）代入这个函数，便能得到此人的死宅分数预测score了。

但怎样得出这个函数呢？通过数据点出发，我们能得到很多个方程组成的一个大方程组：

```
ux1 + vy1 + wz1 = score1 
ux2 + vx2 + wz2 = score2
...
```

但这样似乎不太对--我们凭什么断定，这些数据点在数据空间里的位置纯粹由x，y，z来决定呢？我们为什么能断定，没有什么不受这三个IP评分影响的某种东西同样在影响着一个人的死宅程度呢？我们不能断定，因此我们还要加入一个常数项，用来描述在以x，y，z作为特征主导我们的判断的情况下，所遗漏掉的其他影响因素（这些因素不随x，y，z而变，因此可以写为常数），如下所示：

```
b + ux1 + vy1 + wz1 = score1
b + ux2 + vx2 + wz2 = score2
...
```

我们得到了方程组，然后呢？我们自然是要解这个方程组，但我们要得到的并不是x，y，z，score中的任何一个，我们要得到的是构成我们需要的函数的参数，也就是b，u，v，w，这些参数是我们真正要求解的东西。我们的目标也因此转移为了解一个硕大的三元一次方程组。由于这个方程组太过巨大，我们此处先列举上四个，并且列出它们的矩阵形式。

```
b + ux1 + vy1 + wz1 = score1
b + ux2 + vx2 + wz2 = score2
b + ux3 + vy3 + wz3 = score3
b + ux4 + vx4 + wz4 = score4
```

也就是：

```
1 x1 y1 z1   b   score1
1 x2 y2 z2 * u = score2
1 x3 y3 z3   v   score3
1 x4 y4 z4   w   score4
```

它自然可以写作 `Ax = b` 的形式。

显然，如果对矩阵消元，我们是可以得到这个方程组的唯一解的，前提是，这个矩阵的各行各列都线性无关。想象一下，一个可能有数万行列的矩阵，各行各列都线性无关的概率非常小，甚至于大概率是无解的。因此，我们需要稍微改变下我们的目标，我们需要从求解，改为求当前（无解）情况下的最优解。但是何为最优解？最优解便是，当我们把矩阵A当作b，u，v，w到向量score的线性映射，而这个线性映射无解，也就是得不到当前的向量score之后，我们将原本的向量score分解为两个向量err和新的score，这个新的score落在了原本方程组的解域之中，也就是矩阵A的列空间之中，我们用新的score来代替原本不可能达到的那个score，但这个新score此时只能说是替代解而非最优解，如何得到最优解？我们便可以看向量的分解，想让新的score最接近原来的score，就要让另一个分量err最小--而什么时候err最小呢？答案是新的score和err刚好正交的时候，也就是说，新的score是原本score向量在矩阵A列空间上的投影的时候。也就是说，求方程组Ax=b的最优解，即是求b在矩阵A列空间上投影的过程。

那么，在线性代数中，自然有一个求解向量b在矩阵A列空间投影的公式，那便是：p = （A（ATA）-1 AT）b，由此，我们便求出了p，而由其初始形态 x = （ATA）-1 AT b，我们也能得到x，即向量b，u，v，w，问题得解。

解决了b，u，v，w，我们便能得到我们想要的那个函数，从而根据手游评分来预测未知个体的死宅浓度了。

## 2. 对向量的诠释
都说线性代数围绕的是名为 **向量(Vector)** 的东西，但究竟什么是向量？

首先，向量可以被理解为空间中一个具有方向和长度的箭头（事实上，在计算机视觉中，我们用卷积操作得到图像中像素的梯度时，也要有梯度方向和梯度幅值这两个部分，想来这些像素梯度应该也可以用向量来表示），其次，从一个更加机器学习的角度来看，向量是一个有序的数字列表，它在大多数情况下表示数据所描述事物的不同特征。但从更纯粹的数学角度上来看，向量实际上可以是任何东西，只要它能够被放进一个 **向量空间(Vector Space)** 里面。

那么，什么是向量空间？向量空间可以是容纳任何东西的一个空间，只要里面的东西能够满足**相加**和**数乘**两个运算（目前，我们先想象一个向量空间的例子，即一个拥有x，y坐标轴的二维空间，里面容纳的是以原点为起始，落于任意坐标`[a, b]`的箭头v）-- 向量的相加意味着将两个数组内对应序号的数字相加，也意味着将两个箭头以平行四边形法则相接得到的对角线，向量的数乘则是对箭头长度的缩放，以及对数组内所有数同步的缩放。不要忘了，这两者（数字列表和原点起始的箭头）可以随时转换，这也是向量之所以广泛用于数据分析的原因之一--它可以将庞大，繁杂的数据转化为直观简洁的几何形式。

## 3. 矩阵与线性映射

在进行这一部分的讨论之前，我们首先要知道，一个问题，或者一个数学变换，怎样才是“线性”的？

首先，此处我们对数学变换的定义近似为一个函数：有一个数学变换f，它能够接收一个x，并返回一个变换后的y。而在这个有关数学变换的定义的基础上，对于一个线性变换来说，它在加法和数乘运算上封闭，即满足：

```
f(x1 + x2) = f(x1) + f(x2)
f(cx) = cf(x) 
```

以一条经过原点的直线 f(x) = 2x 作为例子，这条直线就是一个线性变换，因为它满足`f(1 + 2) = 2 * 3 = f(1) + f(2) = 2 * 1 + 2 * 3`，并且满足`f(3 * 1) = 3 = 3 * f(1)`，但如果我们将f(x)变为f(x) = 2x + 1，或者f(x) = x^2，这个变换就不在加法和数乘上封闭，也不是一个线性变换了（2x+1的图像虽然是一个直线，但其并不是一个线性变换，而是在经过原点的直线上添加了一个平移，为一个**仿射变换**）。 至于怎样判断一个潜在的数学变换是不是线性变换，相比我们只有先假设它是，而后再根据上面两个性质来反推其存在了吧。

我们从要解决的问题背景中，便能看出某个地方似乎经过了某种“线性变换”。举一个利用线性变换的性质或原理解决现实问题的例子，那便是知名的“鸡兔同笼问题”情景：已知在一个笼子里有若干只鸡，若干只兔子，但我们知道的是笼子中一共有10个头，28只脚（当然，我们还知道一只鸡有1个头2只脚，一只兔子有1个头4只脚），通过这个问题情景，我们的任务便是解出笼子中一共有多少只鸡，多少只兔子。

这个问题中隐含着一个数学变换，而且是线性变换，那便是“一只鸡有一头两脚，一只兔有一头四脚”，即：

```
fchicken(x) = [x, 2x] = [1, 2] * x
frabbit(x) = [x, 4x] = [1, 4] * x
```

甚至于，我们可以跳出鸡和兔的区分，将它们两个的数量以及头，脚数量整合在一起，形成一个更大的线性变换--这也是线性变换的一个特征，通常来说，一个大的线性变换可以被拆分为数个小的线性变换，而多个小的线性变换也可以整合为一个大的线性变换。想要证明它是线性变换也很简单，因为它们在加法和数乘运算上封闭--显然，两只鸡摆在一起不可能会发生三个头五只脚一类的事情，对于兔子也一样，而对着一只鸡数两次头和脚的数量也显然不会突然发现多了一个头或少了一只脚的情况，而当你把鸡和兔一起数，数出来的结果肯定也不会和分开数有所不同。

```
f([chicken, rabbit]) = fchicken(chicken) + frabbit(rabbit)
                     = [1, 2] * chicken + [1, 4] * rabbit
                     = [heads, feet]
```

在线性代数的背景下，这样一个线性变换也会被称作一个**线性映射（Linear Map）**，表示一个向量`[chicken, rabbit]`经过了变换f，从原来所属的向量空间被映射到了另一个向量空间，在此空间中这个向量变为了`[heads, feet]`的模样。而从前面对于`f([chicken, rabbit])`的表示我们可以看出，这个线性变换内部似乎也存在两个向量，其与输入向量相乘得到输出向量的过程俨然可以被写作一个矩阵乘以向量的过程，即：

```
A = [1,1]     
    [2,4]
A * [chicken, rabbit] = [heads, feet] 
```

